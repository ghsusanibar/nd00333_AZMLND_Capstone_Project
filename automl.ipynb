{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated ML\n",
    "\n",
    "TODO: Import Dependencies. In the cell below, import all the dependencies that you will need to complete the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1598423888013
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.18.0\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.pipeline.steps import AutoMLStep\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import pkg_resources\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, boxcox_normmax\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset\n",
    "\n",
    "### 1.1 Overview\n",
    "\n",
    "✅ In this notebook we are going to use the Cardiovascular Disease dataset from Kaggle. Cardiovascular Disease dataset is a Kaggle Dataset the containts history of health status of some persons. A group of them suffered a heart attackt. So using this dataset we can train a model in order to predict if a person could suffer a heart attack.\n",
    "\n",
    "We can download the data from Kaggle page (https://www.kaggle.com/sulianova/cardiovascular-disease-dataset). In this case, I've download the data in the /data directory. So then we have to register this Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18393</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20228</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  \\\n",
       "0   0  18393       2     168    62.0    110     80            1     1      0   \n",
       "1   1  20228       1     156    85.0    140     90            3     1      0   \n",
       "\n",
       "   alco  active  cardio  \n",
       "0     0       1       0  \n",
       "1     0       1       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileCardioData = 'kaggle/cardio_train.csv'\n",
    "df = pd.read_csv(fileCardioData, encoding='latin')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to local folder\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "dataDir = 'data'\n",
    "\n",
    "if not os.path.exists(dataDir):\n",
    "    os.mkdir(dataDir)\n",
    "\n",
    "fileData = dataDir + \"/initialfile.parquet\"\n",
    "\n",
    "df.to_csv(fileData, index=False)\n",
    "\n",
    "print(\"Data written to local folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Upload to Azure Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace: quick-starts-ws-126148\n",
      "Region: southcentralus\n",
      "Uploading an estimated of 1 files\n",
      "Uploading data/initialfile.parquet\n",
      "Uploaded data/initialfile.parquet, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n",
      "Upload completed\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(\"Workspace: \" + ws.name, \"Region: \" + ws.location, sep = '\\n')\n",
    "\n",
    "# Default datastore\n",
    "default_store = ws.get_default_datastore() \n",
    "\n",
    "default_store.upload_files([fileData], \n",
    "                           target_path = 'cardio', \n",
    "                           overwrite = True, \n",
    "                           show_progress = True)\n",
    "\n",
    "print(\"Upload completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create and register datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "cardio_data = Dataset.Tabular.from_delimited_files(default_store.path('cardio/initialfile.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardio_data = cardio_data.register(ws, 'cardio_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "amlcompute_cluster_name = \"compt-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    aml_compute = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           max_nodes=4)\n",
    "    aml_compute = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
    "\n",
    "aml_compute.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run configuration created.\n"
     ]
    }
   ],
   "source": [
    "# Define RunConfig for the compute\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Create a new runconfig object\n",
    "aml_run_config = RunConfiguration()\n",
    "\n",
    "# Use the aml_compute you created above. \n",
    "aml_run_config.target = aml_compute\n",
    "\n",
    "# Enable Docker\n",
    "aml_run_config.environment.docker.enabled = True\n",
    "\n",
    "# Use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "aml_run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# Specify CondaDependencies obj, add necessary packages\n",
    "aml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\n",
    "    conda_packages=['pandas','scikit-learn','numpy'], \n",
    "    pip_packages=['azureml-sdk[automl,explain]', 'scipy'])\n",
    "\n",
    "print (\"Run configuration created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial columns to use\n",
    "cols_touse = str(['age', 'height', 'weight', 'ap_hi', 'ap_lo',\n",
    "       'cholesterol', 'gluc', 'smoke', 'alco', 'active', 'cardio']).replace(\",\", \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning script is in /mnt/batch/tasks/shared/LS_root/mounts/clusters/compt-inst/code/nd00333_AZMLND_Capstone_Project/scripts.\n",
      "Clean Step created\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "# python scripts folder\n",
    "prepare_data_folder = './scripts'\n",
    "\n",
    "# Define output after cleansing step\n",
    "cleaned_data = PipelineData(\"cleaned_data\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Cleaning script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# Cleaning step creation\n",
    "cleaningStep = PythonScriptStep(\n",
    "    name=\"Clean Data\",\n",
    "    script_name=\"clean.py\", \n",
    "    arguments=[\"--useful_columns\", cols_touse,\n",
    "               \"--output_clean\", cleaned_data],\n",
    "    inputs=[cardio_data.as_named_input('raw_data')],\n",
    "    outputs=[cleaned_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig=aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"Clean Step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter script is in /mnt/batch/tasks/shared/LS_root/mounts/clusters/compt-inst/code/nd00333_AZMLND_Capstone_Project/scripts.\n",
      "Filter Step created\n"
     ]
    }
   ],
   "source": [
    "# Define output after merging step\n",
    "filtered_data = PipelineData(\"filtered_data\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Filter script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# filter step creation\n",
    "# See the filter.py for details about input and output\n",
    "filterStep = PythonScriptStep(\n",
    "    name=\"Filter Data\",\n",
    "    script_name=\"filter.py\", \n",
    "    arguments=[\"--output_filter\", filtered_data],\n",
    "    inputs=[cleaned_data.parse_parquet_files()],\n",
    "    outputs=[filtered_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig = aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"Filter Step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform script is in /mnt/batch/tasks/shared/LS_root/mounts/clusters/compt-inst/code/nd00333_AZMLND_Capstone_Project/scripts.\n",
      "Transform Step created\n"
     ]
    }
   ],
   "source": [
    "# Define output after transform step\n",
    "transformed_data = PipelineData(\"transformed_data\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Transform script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# transform step creation\n",
    "# See the transform.py for details about input and output\n",
    "transformStep = PythonScriptStep(\n",
    "    name=\"Transform Data\",\n",
    "    script_name=\"transform.py\", \n",
    "    arguments=[\"--output_transform\", transformed_data],\n",
    "    inputs=[filtered_data.parse_parquet_files()],\n",
    "    outputs=[transformed_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig = aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"Transform Step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Split Data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data spilt script is in /mnt/batch/tasks/shared/LS_root/mounts/clusters/compt-inst/code/nd00333_AZMLND_Capstone_Project/scripts.\n",
      "TrainTest Split Step created\n"
     ]
    }
   ],
   "source": [
    "# train and test splits output\n",
    "output_split_train = PipelineData(\"output_split_train\", datastore=default_store).as_dataset()\n",
    "output_split_test = PipelineData(\"output_split_test\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Data spilt script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# test train split step creation\n",
    "# See the train_test_split.py for details about input and output\n",
    "testTrainSplitStep = PythonScriptStep(\n",
    "    name=\"Train Test Data Split\",\n",
    "    script_name=\"train_test_split.py\", \n",
    "    arguments=[\"--output_split_train\", output_split_train,\n",
    "               \"--output_split_test\", output_split_test],\n",
    "    inputs=[transformed_data.parse_parquet_files()],\n",
    "    outputs=[output_split_train, output_split_test],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig = aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"TrainTest Split Step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. AutoML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment created\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment = Experiment(ws, 'AutoML-Pipeline')\n",
    "\n",
    "print(\"Experiment created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 AutmoML Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML config created\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "automl_settings = {\n",
    "    \"experiment_timeout_minutes\": 30,\n",
    "    \"max_concurrent_iterations\": 5,\n",
    "    \"primary_metric\" : 'AUC_weighted',\n",
    "    \"n_cross_validations\": 5\n",
    "}\n",
    "\n",
    "training_dataset = output_split_train.parse_parquet_files()\n",
    "\n",
    "automl_config = AutoMLConfig(compute_target=aml_compute,\n",
    "                             task = \"classification\",\n",
    "                             training_data=training_dataset,\n",
    "                             label_column_name=\"cardio\",  \n",
    "                             path = prepare_data_folder,\n",
    "                             enable_early_stopping= True,\n",
    "                             featurization= 'auto',#\n",
    "                             debug_log = \"automl_errors.log\",\n",
    "                             **automl_settings\n",
    "                            )\n",
    "\n",
    "print(\"AutoML config created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData, TrainingOutput\n",
    "\n",
    "ds = ws.get_default_datastore()\n",
    "metrics_output_name = 'metrics_output'\n",
    "best_model_output_name = 'best_model_output'\n",
    "\n",
    "metrics_data = PipelineData(name='metrics_data',\n",
    "                           datastore=ds,\n",
    "                           pipeline_output_name=metrics_output_name,\n",
    "                           training_output=TrainingOutput(type='Metrics'))\n",
    "model_data = PipelineData(name='model_data',\n",
    "                           datastore=ds,\n",
    "                           pipeline_output_name=best_model_output_name,\n",
    "                           training_output=TrainingOutput(type='Model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainWithAutomlStep created\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.steps import AutoMLStep\n",
    "\n",
    "trainWithAutomlStep = AutoMLStep(name='AutoML_Classification',\n",
    "                                 outputs=[metrics_data, model_data],\n",
    "                                 automl_config=automl_config,\n",
    "                                 allow_reuse=True)\n",
    "print(\"trainWithAutomlStep created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline is built.\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "pipeline_steps = [trainWithAutomlStep]\n",
    "\n",
    "pipeline = Pipeline(workspace = ws, steps=pipeline_steps)\n",
    "print(\"Pipeline is built.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step AutoML_Classification [f4927756][778ffac1-b358-41ea-91df-cf804cea91ea], (This step will run and generate new outputs)\n",
      "Created step Train Test Data Split [f4ea6362][ae9fc609-86a6-433c-b508-9998fd640a4e], (This step will run and generate new outputs)\n",
      "Created step Transform Data [b6762a56][b063381e-a44c-47a2-bfda-93964decaab4], (This step will run and generate new outputs)\n",
      "Created step Filter Data [dda62dd1][900d88db-c9c5-492d-a230-ae54150a249f], (This step will run and generate new outputs)\n",
      "Created step Clean Data [792d5cb7][19603df0-4468-4d26-bfca-225a5f7537cc], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun ad0df4a7-dae2-4078-8b49-48e0590a285b\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/AutoML-Pipeline/runs/ad0df4a7-dae2-4078-8b49-48e0590a285b?wsid=/subscriptions/4910dccd-0348-46c4-a51f-d8c85e078b14/resourcegroups/aml-quickstarts-126148/workspaces/quick-starts-ws-126148\n",
      "Pipeline submitted for execution.\n"
     ]
    }
   ],
   "source": [
    "pipeline_run = experiment.submit(pipeline, regenerate_outputs=False)\n",
    "\n",
    "print(\"Pipeline submitted for execution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 RunDetails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9af59dba699474da0cff9cdb08239b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Completed\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/AutoML-Pipeline/runs/ad0df4a7-dae2-4078-8b49-48e0590a285b?wsid=/subscriptions/4910dccd-0348-46c4-a51f-d8c85e078b14/resourcegroups/aml-quickstarts-126148/workspaces/quick-starts-ws-126148\", \"run_id\": \"ad0df4a7-dae2-4078-8b49-48e0590a285b\", \"run_properties\": {\"run_id\": \"ad0df4a7-dae2-4078-8b49-48e0590a285b\", \"created_utc\": \"2020-11-14T16:24:37.575518Z\", \"properties\": {\"azureml.runsource\": \"azureml.PipelineRun\", \"runSource\": \"SDK\", \"runType\": \"SDK\", \"azureml.parameters\": \"{}\"}, \"tags\": {\"azureml.pipelineComponent\": \"pipelinerun\"}, \"end_time_utc\": \"2020-11-14T17:27:45.593083Z\", \"status\": \"Completed\", \"log_files\": {\"logs/azureml/executionlogs.txt\": \"https://mlstrg126148.blob.core.windows.net/azureml/ExperimentRun/dcid.ad0df4a7-dae2-4078-8b49-48e0590a285b/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=NKKBkNCjOMFTt6VfcIwwXbhwSidthAzoR8ipQx2D5fE%3D&st=2020-11-14T17%3A15%3A54Z&se=2020-11-15T01%3A25%3A54Z&sp=r\", \"logs/azureml/stderrlogs.txt\": \"https://mlstrg126148.blob.core.windows.net/azureml/ExperimentRun/dcid.ad0df4a7-dae2-4078-8b49-48e0590a285b/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=XYtLQhNJjBdN7n%2BYlKOLmCHOy9hp4sv5L6kfegEmapQ%3D&st=2020-11-14T17%3A15%3A54Z&se=2020-11-15T01%3A25%3A54Z&sp=r\", \"logs/azureml/stdoutlogs.txt\": \"https://mlstrg126148.blob.core.windows.net/azureml/ExperimentRun/dcid.ad0df4a7-dae2-4078-8b49-48e0590a285b/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=tknnVJusZzQGSDKEL%2BwOA%2BJ3VxKM6kumf8NpczZcGt4%3D&st=2020-11-14T17%3A15%3A54Z&se=2020-11-15T01%3A25%3A54Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/executionlogs.txt\", \"logs/azureml/stderrlogs.txt\", \"logs/azureml/stdoutlogs.txt\"]], \"run_duration\": \"1:03:08\"}, \"child_runs\": [{\"run_id\": \"3bc44d41-0d20-4691-885c-e541832b3c80\", \"name\": \"AutoML_Classification\", \"status\": \"Finished\", \"start_time\": \"2020-11-14T16:31:12.82244Z\", \"created_time\": \"2020-11-14T16:30:39.026779Z\", \"end_time\": \"2020-11-14T17:22:16.940658Z\", \"duration\": \"0:51:37\", \"run_number\": 20, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-11-14T16:30:39.026779Z\", \"is_reused\": \"\"}, {\"run_id\": \"c9f5af48-9b5d-4058-8738-58fc8df29abc\", \"name\": \"Train Test Data Split\", \"status\": \"Finished\", \"start_time\": \"2020-11-14T16:29:29.873592Z\", \"created_time\": \"2020-11-14T16:29:13.925612Z\", \"end_time\": \"2020-11-14T16:30:27.221282Z\", \"duration\": \"0:01:13\", \"run_number\": 19, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-11-14T16:29:13.925612Z\", \"is_reused\": \"\"}, {\"run_id\": \"58f9fd12-911e-4335-bc74-35ff63273575\", \"name\": \"Transform Data\", \"status\": \"Finished\", \"start_time\": \"2020-11-14T16:28:00.284837Z\", \"created_time\": \"2020-11-14T16:27:23.230257Z\", \"end_time\": \"2020-11-14T16:29:03.47373Z\", \"duration\": \"0:01:40\", \"run_number\": 18, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-11-14T16:27:23.230257Z\", \"is_reused\": \"\"}, {\"run_id\": \"b1eaca40-fd19-4ec1-9d73-6596302b766a\", \"name\": \"Filter Data\", \"status\": \"Finished\", \"start_time\": \"2020-11-14T16:26:18.85359Z\", \"created_time\": \"2020-11-14T16:26:03.199508Z\", \"end_time\": \"2020-11-14T16:27:17.315922Z\", \"duration\": \"0:01:14\", \"run_number\": 17, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-11-14T16:26:03.199508Z\", \"is_reused\": \"\"}, {\"run_id\": \"dc895ee2-900c-461f-9668-aed4bf793c2f\", \"name\": \"Clean Data\", \"status\": \"Finished\", \"start_time\": \"2020-11-14T16:25:01.7702Z\", \"created_time\": \"2020-11-14T16:24:48.594341Z\", \"end_time\": \"2020-11-14T16:25:58.425271Z\", \"duration\": \"0:01:09\", \"run_number\": 16, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-11-14T16:24:48.594341Z\", \"is_reused\": \"\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2020-11-14 16:24:48Z] Submitting 1 runs, first five are: 792d5cb7:dc895ee2-900c-461f-9668-aed4bf793c2f\\n[2020-11-14 16:26:02Z] Completing processing run id dc895ee2-900c-461f-9668-aed4bf793c2f.\\n[2020-11-14 16:26:03Z] Submitting 1 runs, first five are: dda62dd1:b1eaca40-fd19-4ec1-9d73-6596302b766a\\n[2020-11-14 16:27:22Z] Completing processing run id b1eaca40-fd19-4ec1-9d73-6596302b766a.\\n[2020-11-14 16:27:23Z] Submitting 1 runs, first five are: b6762a56:58f9fd12-911e-4335-bc74-35ff63273575\\n[2020-11-14 16:29:13Z] Completing processing run id 58f9fd12-911e-4335-bc74-35ff63273575.\\n[2020-11-14 16:29:13Z] Submitting 1 runs, first five are: f4ea6362:c9f5af48-9b5d-4058-8738-58fc8df29abc\\n[2020-11-14 16:30:38Z] Completing processing run id c9f5af48-9b5d-4058-8738-58fc8df29abc.\\n[2020-11-14 16:30:38Z] Submitting 1 runs, first five are: f4927756:3bc44d41-0d20-4691-885c-e541832b3c80\\n[2020-11-14 17:27:45Z] Completing processing run id 3bc44d41-0d20-4691-885c-e541832b3c80.\\n\\nRun is completed.\", \"graph\": {\"datasource_nodes\": {\"421fe84a\": {\"node_id\": \"421fe84a\", \"name\": \"cardio_data\"}}, \"module_nodes\": {\"f4927756\": {\"node_id\": \"f4927756\", \"name\": \"AutoML_Classification\", \"status\": \"Finished\", \"_is_reused\": false, \"run_id\": \"3bc44d41-0d20-4691-885c-e541832b3c80\"}, \"f4ea6362\": {\"node_id\": \"f4ea6362\", \"name\": \"Train Test Data Split\", \"status\": \"Finished\", \"_is_reused\": false, \"run_id\": \"c9f5af48-9b5d-4058-8738-58fc8df29abc\"}, \"b6762a56\": {\"node_id\": \"b6762a56\", \"name\": \"Transform Data\", \"status\": \"Finished\", \"_is_reused\": false, \"run_id\": \"58f9fd12-911e-4335-bc74-35ff63273575\"}, \"dda62dd1\": {\"node_id\": \"dda62dd1\", \"name\": \"Filter Data\", \"status\": \"Finished\", \"_is_reused\": false, \"run_id\": \"b1eaca40-fd19-4ec1-9d73-6596302b766a\"}, \"792d5cb7\": {\"node_id\": \"792d5cb7\", \"name\": \"Clean Data\", \"status\": \"Finished\", \"_is_reused\": false, \"run_id\": \"dc895ee2-900c-461f-9668-aed4bf793c2f\"}}, \"edges\": [{\"source_node_id\": \"f4ea6362\", \"source_node_name\": \"Train Test Data Split\", \"source_name\": \"output_split_train\", \"target_name\": \"training_data\", \"dst_node_id\": \"f4927756\", \"dst_node_name\": \"AutoML_Classification\"}, {\"source_node_id\": \"b6762a56\", \"source_node_name\": \"Transform Data\", \"source_name\": \"transformed_data\", \"target_name\": \"transformed_data\", \"dst_node_id\": \"f4ea6362\", \"dst_node_name\": \"Train Test Data Split\"}, {\"source_node_id\": \"dda62dd1\", \"source_node_name\": \"Filter Data\", \"source_name\": \"filtered_data\", \"target_name\": \"filtered_data\", \"dst_node_id\": \"b6762a56\", \"dst_node_name\": \"Transform Data\"}, {\"source_node_id\": \"792d5cb7\", \"source_node_name\": \"Clean Data\", \"source_name\": \"cleaned_data\", \"target_name\": \"cleaned_data\", \"dst_node_id\": \"dda62dd1\", \"dst_node_name\": \"Filter Data\"}, {\"source_node_id\": \"421fe84a\", \"source_node_name\": \"cardio_data\", \"source_name\": \"data\", \"target_name\": \"raw_data\", \"dst_node_id\": \"792d5cb7\", \"dst_node_name\": \"Clean Data\"}], \"child_runs\": [{\"run_id\": \"3bc44d41-0d20-4691-885c-e541832b3c80\", \"name\": \"AutoML_Classification\", \"status\": \"Finished\", \"start_time\": \"2020-11-14T16:31:12.82244Z\", \"created_time\": \"2020-11-14T16:30:39.026779Z\", \"end_time\": \"2020-11-14T17:22:16.940658Z\", \"duration\": \"0:51:37\", \"run_number\": 20, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-11-14T16:30:39.026779Z\", \"is_reused\": \"\"}, {\"run_id\": \"c9f5af48-9b5d-4058-8738-58fc8df29abc\", \"name\": \"Train Test Data Split\", \"status\": \"Finished\", \"start_time\": \"2020-11-14T16:29:29.873592Z\", \"created_time\": \"2020-11-14T16:29:13.925612Z\", \"end_time\": \"2020-11-14T16:30:27.221282Z\", \"duration\": \"0:01:13\", \"run_number\": 19, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-11-14T16:29:13.925612Z\", \"is_reused\": \"\"}, {\"run_id\": \"58f9fd12-911e-4335-bc74-35ff63273575\", \"name\": \"Transform Data\", \"status\": \"Finished\", \"start_time\": \"2020-11-14T16:28:00.284837Z\", \"created_time\": \"2020-11-14T16:27:23.230257Z\", \"end_time\": \"2020-11-14T16:29:03.47373Z\", \"duration\": \"0:01:40\", \"run_number\": 18, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-11-14T16:27:23.230257Z\", \"is_reused\": \"\"}, {\"run_id\": \"b1eaca40-fd19-4ec1-9d73-6596302b766a\", \"name\": \"Filter Data\", \"status\": \"Finished\", \"start_time\": \"2020-11-14T16:26:18.85359Z\", \"created_time\": \"2020-11-14T16:26:03.199508Z\", \"end_time\": \"2020-11-14T16:27:17.315922Z\", \"duration\": \"0:01:14\", \"run_number\": 17, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-11-14T16:26:03.199508Z\", \"is_reused\": \"\"}, {\"run_id\": \"dc895ee2-900c-461f-9668-aed4bf793c2f\", \"name\": \"Clean Data\", \"status\": \"Finished\", \"start_time\": \"2020-11-14T16:25:01.7702Z\", \"created_time\": \"2020-11-14T16:24:48.594341Z\", \"end_time\": \"2020-11-14T16:25:58.425271Z\", \"duration\": \"0:01:09\", \"run_number\": 16, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-11-14T16:24:48.594341Z\", \"is_reused\": \"\"}]}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.18.0\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineRunId: ad0df4a7-dae2-4078-8b49-48e0590a285b\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/AutoML-Pipeline/runs/ad0df4a7-dae2-4078-8b49-48e0590a285b?wsid=/subscriptions/4910dccd-0348-46c4-a51f-d8c85e078b14/resourcegroups/aml-quickstarts-126148/workspaces/quick-starts-ws-126148\n",
      "{'runId': 'ad0df4a7-dae2-4078-8b49-48e0590a285b', 'status': 'Completed', 'startTimeUtc': '2020-11-14T16:24:43.465765Z', 'endTimeUtc': '2020-11-14T17:27:45.593083Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{}'}, 'inputDatasets': [], 'outputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://mlstrg126148.blob.core.windows.net/azureml/ExperimentRun/dcid.ad0df4a7-dae2-4078-8b49-48e0590a285b/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=NKKBkNCjOMFTt6VfcIwwXbhwSidthAzoR8ipQx2D5fE%3D&st=2020-11-14T17%3A15%3A54Z&se=2020-11-15T01%3A25%3A54Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://mlstrg126148.blob.core.windows.net/azureml/ExperimentRun/dcid.ad0df4a7-dae2-4078-8b49-48e0590a285b/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=XYtLQhNJjBdN7n%2BYlKOLmCHOy9hp4sv5L6kfegEmapQ%3D&st=2020-11-14T17%3A15%3A54Z&se=2020-11-15T01%3A25%3A54Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://mlstrg126148.blob.core.windows.net/azureml/ExperimentRun/dcid.ad0df4a7-dae2-4078-8b49-48e0590a285b/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=tknnVJusZzQGSDKEL%2BwOA%2BJ3VxKM6kumf8NpczZcGt4%3D&st=2020-11-14T17%3A15%3A54Z&se=2020-11-15T01%3A25%3A54Z&sp=r'}}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before we proceed we need to wait for the run to complete.\n",
    "pipeline_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Explore Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Best Model\n",
    "\n",
    "TODO: In the cell below, get the best model from the automl experiments and display all the properties of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431425670
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431426111
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Save the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "Remember you have to deploy only one of the two models you trained.. Perform the steps in the rest of this notebook only if you wish to deploy this model.\n",
    "\n",
    "TODO: In the cell below, register the model, create an inference config and deploy the model as a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431435189
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598431657736
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, send a request to the web service you deployed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598432707604
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598432765711
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, print the logs of the web service and delete the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
