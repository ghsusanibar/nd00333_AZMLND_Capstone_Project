{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning using HyperDrive\n",
    "\n",
    "TODO: Import Dependencies. In the cell below, import all the dependencies that you will need to complete the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1598531914256
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.18.0\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Datastore, Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.pipeline.steps import HyperDriveStep, HyperDriveStepRun, PythonScriptStep\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, TrainingOutput\n",
    "# from azureml.train.hyperdrive import *\n",
    "from azureml.train.hyperdrive import RandomParameterSampling, BanditPolicy, HyperDriveConfig, PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive import choice, loguniform\n",
    "from azureml.train.hyperdrive.parameter_expressions import uniform\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import urllib\n",
    "import pkg_resources\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import logging\n",
    "import csv\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, boxcox_normmax\n",
    "from scipy.special import boxcox1p\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset\n",
    "\n",
    "### 1.1 Overview\n",
    "\n",
    "âœ… In this notebook we are going to use the Cardiovascular Disease dataset from Kaggle. Cardiovascular Disease dataset is a Kaggle Dataset the containts history of health status of some persons. A group of them suffered a heart attackt. So using this dataset we can train a model in order to predict if a person could suffer a heart attack.\n",
    "\n",
    "We can download the data from Kaggle page (https://www.kaggle.com/sulianova/cardiovascular-disease-dataset). In this case, I've download the data in the /data directory. So then we have to register this Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18393</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20228</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  \\\n",
       "0   0  18393       2     168    62.0    110     80            1     1      0   \n",
       "1   1  20228       1     156    85.0    140     90            3     1      0   \n",
       "\n",
       "   alco  active  cardio  \n",
       "0     0       1       0  \n",
       "1     0       1       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileCardioData = 'kaggle/cardio_train.csv'\n",
    "df = pd.read_csv(fileCardioData, encoding='latin')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to local folder\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "dataDir = 'data'\n",
    "\n",
    "if not os.path.exists(dataDir):\n",
    "    os.mkdir(dataDir)\n",
    "\n",
    "fileData = dataDir + \"/initialfile.parquet\"\n",
    "\n",
    "df.to_csv(fileData, index=False)\n",
    "\n",
    "print(\"Data written to local folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Upload to Azure Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace: quick-starts-ws-126184\n",
      "Region: southcentralus\n",
      "Uploading an estimated of 1 files\n",
      "Uploading data/initialfile.parquet\n",
      "Uploaded data/initialfile.parquet, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n",
      "Upload completed\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(\"Workspace: \" + ws.name, \"Region: \" + ws.location, sep = '\\n')\n",
    "\n",
    "# Default datastore\n",
    "default_store = ws.get_default_datastore() \n",
    "\n",
    "default_store.upload_files([fileData], \n",
    "                           target_path = 'cardio', \n",
    "                           overwrite = True, \n",
    "                           show_progress = True)\n",
    "\n",
    "print(\"Upload completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create and register datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "cardio_data = Dataset.Tabular.from_delimited_files(default_store.path('cardio/initialfile.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardio_data = cardio_data.register(ws, 'cardio_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "amlcompute_cluster_name = \"compt-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    aml_compute = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           max_nodes=4)\n",
    "    aml_compute = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
    "\n",
    "aml_compute.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run configuration created.\n"
     ]
    }
   ],
   "source": [
    "# Define RunConfig for the compute\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Create a new runconfig object\n",
    "aml_run_config = RunConfiguration()\n",
    "\n",
    "# Use the aml_compute you created above. \n",
    "aml_run_config.target = aml_compute\n",
    "\n",
    "# Enable Docker\n",
    "aml_run_config.environment.docker.enabled = True\n",
    "\n",
    "# Use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "aml_run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# Specify CondaDependencies obj, add necessary packages\n",
    "aml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\n",
    "    conda_packages=['pandas','scikit-learn','numpy'], \n",
    "    pip_packages=['azureml-sdk[automl,explain]', 'scipy','lightgbm'])\n",
    "\n",
    "print (\"Run configuration created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial columns to use\n",
    "cols_touse = str(['age', 'height', 'weight', 'ap_hi', 'ap_lo',\n",
    "       'cholesterol', 'gluc', 'smoke', 'alco', 'active', 'cardio']).replace(\",\", \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning script is in /mnt/batch/tasks/shared/LS_root/mounts/clusters/compt-inst/code/nd00333_AZMLND_Capstone_Project/scripts.\n",
      "Clean Step created\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "# python scripts folder\n",
    "prepare_data_folder = './scripts'\n",
    "\n",
    "# Define output after cleansing step\n",
    "cleaned_data = PipelineData(\"cleaned_data\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Cleaning script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# Cleaning step creation\n",
    "cleaningStep = PythonScriptStep(\n",
    "    name=\"Clean Data\",\n",
    "    script_name=\"clean.py\", \n",
    "    arguments=[\"--useful_columns\", cols_touse,\n",
    "               \"--output_clean\", cleaned_data],\n",
    "    inputs=[cardio_data.as_named_input('raw_data')],\n",
    "    outputs=[cleaned_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig=aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"Clean Step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter script is in /mnt/batch/tasks/shared/LS_root/mounts/clusters/compt-inst/code/nd00333_AZMLND_Capstone_Project/scripts.\n",
      "Filter Step created\n"
     ]
    }
   ],
   "source": [
    "# Define output after merging step\n",
    "filtered_data = PipelineData(\"filtered_data\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Filter script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# filter step creation\n",
    "# See the filter.py for details about input and output\n",
    "filterStep = PythonScriptStep(\n",
    "    name=\"Filter Data\",\n",
    "    script_name=\"filter.py\", \n",
    "    arguments=[\"--output_filter\", filtered_data],\n",
    "    inputs=[cleaned_data.parse_parquet_files()],\n",
    "    outputs=[filtered_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig = aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"Filter Step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform script is in /mnt/batch/tasks/shared/LS_root/mounts/clusters/compt-inst/code/nd00333_AZMLND_Capstone_Project/scripts.\n",
      "Transform Step created\n"
     ]
    }
   ],
   "source": [
    "# Define output after transform step\n",
    "transformed_data = PipelineData(\"transformed_data\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Transform script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# transform step creation\n",
    "# See the transform.py for details about input and output\n",
    "transformStep = PythonScriptStep(\n",
    "    name=\"Transform Data\",\n",
    "    script_name=\"transform.py\", \n",
    "    arguments=[\"--output_transform\", transformed_data],\n",
    "    inputs=[filtered_data.parse_parquet_files()],\n",
    "    outputs=[transformed_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig = aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"Transform Step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Split Data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data spilt script is in /mnt/batch/tasks/shared/LS_root/mounts/clusters/compt-inst/code/nd00333_AZMLND_Capstone_Project/scripts.\n",
      "TrainTest Split Step created\n"
     ]
    }
   ],
   "source": [
    "# train and test splits output\n",
    "output_split_train = PipelineData(\"output_split_train\", datastore=default_store).as_dataset()\n",
    "output_split_test = PipelineData(\"output_split_test\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Data spilt script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# test train split step creation\n",
    "# See the train_test_split.py for details about input and output\n",
    "testTrainSplitStep = PythonScriptStep(\n",
    "    name=\"Train Test Data Split\",\n",
    "    script_name=\"train_test_split.py\", \n",
    "    arguments=[\"--output_split_train\", output_split_train,\n",
    "               \"--output_split_test\", output_split_test],\n",
    "    inputs=[transformed_data.parse_parquet_files()],\n",
    "    outputs=[output_split_train, output_split_test],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig = aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"TrainTest Split Step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. HyperDrive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment created\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment = Experiment(ws, 'HyperDrive-Pipeline')\n",
    "\n",
    "print(\"Experiment created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 HyperDrive config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify parameter sampler\n",
    "### YOUR CODE HERE ###\n",
    "ps = RandomParameterSampling(\n",
    "    {\n",
    "        '--num_leaves': choice([16, 32, 48, 64]),\n",
    "        '--max_depth': choice([3, 4, 5, 6]),\n",
    "        '--min_data_in_leaf': choice([24, 32, 48, 64])\n",
    "    }\n",
    ")\n",
    "\n",
    "# Specify a Policy\n",
    "policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)\n",
    "\n",
    "script_params = {\n",
    "    '--data_folder': ws.get_default_datastore(),\n",
    "}\n",
    "\n",
    "est = Estimator(source_directory=prepare_data_folder,\n",
    "                #script_params=script_params,\n",
    "                compute_target=aml_compute,\n",
    "                entry_script='train_hyperdrive.py',\n",
    "                pip_packages=['scikit-learn','lightgbm']\n",
    ")\n",
    "\n",
    "# Create a HyperDriveConfig using the estimator, hyperparameter sampler, and policy.\n",
    "hyperdrive_config = HyperDriveConfig(\n",
    "    estimator=est,\n",
    "    hyperparameter_sampling=ps,\n",
    "    policy=policy,\n",
    "    primary_metric_name='AUC',\n",
    "    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "    max_total_runs=16,\n",
    "    max_concurrent_runs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 HyperDrive Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HyperDrive step created\n"
     ]
    }
   ],
   "source": [
    "ds = ws.get_default_datastore()\n",
    "metrics_output_name = 'metrics_output'\n",
    "best_model_output_name = 'best_model_output'\n",
    "\n",
    "metrics_data = PipelineData(name='metrics_data',\n",
    "                           datastore=ds,\n",
    "                           pipeline_output_name=metrics_output_name,\n",
    "                           training_output=TrainingOutput(type='Metrics'))\n",
    "model_data = PipelineData(name='model_data',\n",
    "                           datastore=ds,\n",
    "                           pipeline_output_name=best_model_output_name,\n",
    "                           training_output=TrainingOutput(type='Model'))\n",
    "\n",
    "training_dataset = output_split_train.parse_parquet_files()\n",
    "\n",
    "hd_step_name='HyperDrive Module'\n",
    "hd_step = HyperDriveStep(\n",
    "    name=hd_step_name,\n",
    "    hyperdrive_config=hyperdrive_config,\n",
    "    estimator_entry_script_arguments=['--data_folder', ws.get_default_datastore()],\n",
    "    inputs=[training_dataset],\n",
    "    outputs=[metrics_data, model_data],\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"HyperDrive step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - If 'script' has been provided here and a script file name has been specified in 'run_config', 'script' provided in ScriptRunConfig initialization will take precedence.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline is built.\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "pipeline_steps = [hd_step]\n",
    "\n",
    "pipeline = Pipeline(workspace = ws, steps=pipeline_steps)\n",
    "print(\"Pipeline is built.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step HyperDrive Module [96214e7f][44d1990d-f9d8-46c6-bcdb-90c04f285883], (This step will run and generate new outputs)\n",
      "Created step Train Test Data Split [9d1fb451][6d74e5cc-1e37-4002-9c30-7c284fbd4933], (This step is eligible to reuse a previous run's output)\n",
      "Created step Transform Data [44776bbf][f8ffaa46-cc79-42d3-8c0f-ce771aed2e9e], (This step is eligible to reuse a previous run's output)Created step Filter Data [6f5a92a1][a42da7f4-eabc-46ba-be47-714a97acd0ab], (This step is eligible to reuse a previous run's output)\n",
      "\n",
      "Created step Clean Data [f317b48d][0656195e-7a11-473f-bbfb-d46048dd650f], (This step is eligible to reuse a previous run's output)\n",
      "Submitted PipelineRun 81b72bc1-2268-466c-ba06-27407046dd79\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/HyperDrive-Pipeline/runs/81b72bc1-2268-466c-ba06-27407046dd79?wsid=/subscriptions/94e14ad4-bf97-47e8-aae0-f9b85a7befa8/resourcegroups/aml-quickstarts-126184/workspaces/quick-starts-ws-126184\n",
      "Pipeline submitted for execution.\n"
     ]
    }
   ],
   "source": [
    "pipeline_run = experiment.submit(pipeline, regenerate_outputs=False)\n",
    "\n",
    "print(\"Pipeline submitted for execution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 RunDetails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73c65ab174d42bf9337bcb369b0b01a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Completed\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/HyperDrive-Pipeline/runs/81b72bc1-2268-466c-ba06-27407046dd79?wsid=/subscriptions/94e14ad4-bf97-47e8-aae0-f9b85a7befa8/resourcegroups/aml-quickstarts-126184/workspaces/quick-starts-ws-126184\", \"run_id\": \"81b72bc1-2268-466c-ba06-27407046dd79\", \"run_properties\": {\"run_id\": \"81b72bc1-2268-466c-ba06-27407046dd79\", \"created_utc\": \"2020-11-14T23:40:19.432931Z\", \"properties\": {\"azureml.runsource\": \"azureml.PipelineRun\", \"runSource\": \"SDK\", \"runType\": \"SDK\", \"azureml.parameters\": \"{}\"}, \"tags\": {\"azureml.pipelineComponent\": \"pipelinerun\"}, \"end_time_utc\": \"2020-11-14T23:51:04.569113Z\", \"status\": \"Completed\", \"log_files\": {\"logs/azureml/executionlogs.txt\": \"https://mlstrg126184.blob.core.windows.net/azureml/ExperimentRun/dcid.81b72bc1-2268-466c-ba06-27407046dd79/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=NhaJi3VVPRcDHmxXv1Hh3NoSE9a32W%2By%2BGI9AR%2FLQyg%3D&st=2020-11-14T23%3A30%3A46Z&se=2020-11-15T07%3A40%3A46Z&sp=r\", \"logs/azureml/stderrlogs.txt\": \"https://mlstrg126184.blob.core.windows.net/azureml/ExperimentRun/dcid.81b72bc1-2268-466c-ba06-27407046dd79/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=9j4wx2Hny%2Fehh2uHnz05sDPjnGZEcAf%2BhIzt2A8PCDA%3D&st=2020-11-14T23%3A30%3A46Z&se=2020-11-15T07%3A40%3A46Z&sp=r\", \"logs/azureml/stdoutlogs.txt\": \"https://mlstrg126184.blob.core.windows.net/azureml/ExperimentRun/dcid.81b72bc1-2268-466c-ba06-27407046dd79/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=nbECCSOw2IiVVht%2BhzJ2Jd2l2GCiWkLFecKVoz7Rovg%3D&st=2020-11-14T23%3A30%3A46Z&se=2020-11-15T07%3A40%3A46Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/executionlogs.txt\", \"logs/azureml/stderrlogs.txt\", \"logs/azureml/stdoutlogs.txt\"]], \"run_duration\": \"0:10:45\"}, \"child_runs\": [{\"run_id\": \"b0b6da38-099d-460b-9c04-918f29c30047\", \"name\": \"HyperDrive Module\", \"status\": \"Finished\", \"start_time\": \"2020-11-14T23:40:36.383982Z\", \"created_time\": \"2020-11-14T23:40:30.397207Z\", \"end_time\": \"2020-11-14T23:51:00.584088Z\", \"duration\": \"0:10:30\", \"run_number\": 59, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-11-14T23:40:30.397207Z\", \"is_reused\": \"\"}, {\"run_id\": \"0fec38c9-289f-49a2-8f44-c0935b276285\", \"name\": \"Train Test Data Split\", \"status\": \"Finished\", \"start_time\": \"2020-11-14T23:40:29.763776Z\", \"created_time\": \"2020-11-14T23:40:29.763776Z\", \"end_time\": \"2020-11-14T23:40:29.843666Z\", \"duration\": \"0:00:00\", \"run_number\": 58, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-11-14T23:40:29.763776Z\", \"is_reused\": \"Yes\"}, {\"run_id\": \"c6d666a7-89c6-46fd-b6c8-f33f3cc8aebc\", \"name\": \"Transform Data\", \"status\": \"Finished\", \"start_time\": \"2020-11-14T23:40:29.149805Z\", \"created_time\": \"2020-11-14T23:40:29.149805Z\", \"end_time\": \"2020-11-14T23:40:29.22941Z\", \"duration\": \"0:00:00\", \"run_number\": 57, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-11-14T23:40:29.149805Z\", \"is_reused\": \"Yes\"}, {\"run_id\": \"f6bcae15-977b-4844-8cd9-5c2cd087a3ed\", \"name\": \"Filter Data\", \"status\": \"Finished\", \"start_time\": \"2020-11-14T23:40:28.436431Z\", \"created_time\": \"2020-11-14T23:40:28.436431Z\", \"end_time\": \"2020-11-14T23:40:28.530146Z\", \"duration\": \"0:00:00\", \"run_number\": 56, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-11-14T23:40:28.436431Z\", \"is_reused\": \"Yes\"}, {\"run_id\": \"bc650290-8c50-4811-8c4b-b9a57afd980c\", \"name\": \"Clean Data\", \"status\": \"Finished\", \"start_time\": \"2020-11-14T23:40:27.76586Z\", \"created_time\": \"2020-11-14T23:40:27.76586Z\", \"end_time\": \"2020-11-14T23:40:27.856499Z\", \"duration\": \"0:00:00\", \"run_number\": 55, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-11-14T23:40:27.76586Z\", \"is_reused\": \"Yes\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2020-11-14 23:40:27Z] Completing processing run id bc650290-8c50-4811-8c4b-b9a57afd980c.\\n[2020-11-14 23:40:28Z] Completing processing run id f6bcae15-977b-4844-8cd9-5c2cd087a3ed.\\n[2020-11-14 23:40:29Z] Completing processing run id c6d666a7-89c6-46fd-b6c8-f33f3cc8aebc.\\n[2020-11-14 23:40:29Z] Completing processing run id 0fec38c9-289f-49a2-8f44-c0935b276285.\\n[2020-11-14 23:40:30Z] Submitting 1 runs, first five are: 96214e7f:b0b6da38-099d-460b-9c04-918f29c30047\\n[2020-11-14 23:51:04Z] Completing processing run id b0b6da38-099d-460b-9c04-918f29c30047.\\n\\nRun is completed.\", \"graph\": {\"datasource_nodes\": {\"6b7dcbd5\": {\"node_id\": \"6b7dcbd5\", \"name\": \"cardio_data\"}}, \"module_nodes\": {\"96214e7f\": {\"node_id\": \"96214e7f\", \"name\": \"HyperDrive Module\", \"status\": \"Finished\", \"_is_reused\": false, \"run_id\": \"b0b6da38-099d-460b-9c04-918f29c30047\"}, \"9d1fb451\": {\"node_id\": \"9d1fb451\", \"name\": \"Train Test Data Split\", \"status\": \"Finished\", \"_is_reused\": true, \"run_id\": \"0fec38c9-289f-49a2-8f44-c0935b276285\"}, \"44776bbf\": {\"node_id\": \"44776bbf\", \"name\": \"Transform Data\", \"status\": \"Finished\", \"_is_reused\": true, \"run_id\": \"c6d666a7-89c6-46fd-b6c8-f33f3cc8aebc\"}, \"6f5a92a1\": {\"node_id\": \"6f5a92a1\", \"name\": \"Filter Data\", \"status\": \"Finished\", \"_is_reused\": true, \"run_id\": \"f6bcae15-977b-4844-8cd9-5c2cd087a3ed\"}, \"f317b48d\": {\"node_id\": \"f317b48d\", \"name\": \"Clean Data\", \"status\": \"Finished\", \"_is_reused\": true, \"run_id\": \"bc650290-8c50-4811-8c4b-b9a57afd980c\"}}, \"edges\": [{\"source_node_id\": \"9d1fb451\", \"source_node_name\": \"Train Test Data Split\", \"source_name\": \"output_split_train\", \"target_name\": \"output_split_train\", \"dst_node_id\": \"96214e7f\", \"dst_node_name\": \"HyperDrive Module\"}, {\"source_node_id\": \"44776bbf\", \"source_node_name\": \"Transform Data\", \"source_name\": \"transformed_data\", \"target_name\": \"transformed_data\", \"dst_node_id\": \"9d1fb451\", \"dst_node_name\": \"Train Test Data Split\"}, {\"source_node_id\": \"6f5a92a1\", \"source_node_name\": \"Filter Data\", \"source_name\": \"filtered_data\", \"target_name\": \"filtered_data\", \"dst_node_id\": \"44776bbf\", \"dst_node_name\": \"Transform Data\"}, {\"source_node_id\": \"f317b48d\", \"source_node_name\": \"Clean Data\", \"source_name\": \"cleaned_data\", \"target_name\": \"cleaned_data\", \"dst_node_id\": \"6f5a92a1\", \"dst_node_name\": \"Filter Data\"}, {\"source_node_id\": \"6b7dcbd5\", \"source_node_name\": \"cardio_data\", \"source_name\": \"data\", \"target_name\": \"raw_data\", \"dst_node_id\": \"f317b48d\", \"dst_node_name\": \"Clean Data\"}], \"child_runs\": [{\"run_id\": \"b0b6da38-099d-460b-9c04-918f29c30047\", \"name\": \"HyperDrive Module\", \"status\": \"Finished\", \"start_time\": \"2020-11-14T23:40:36.383982Z\", \"created_time\": \"2020-11-14T23:40:30.397207Z\", \"end_time\": \"2020-11-14T23:51:00.584088Z\", \"duration\": \"0:10:30\", \"run_number\": 59, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-11-14T23:40:30.397207Z\", \"is_reused\": \"\"}, {\"run_id\": \"0fec38c9-289f-49a2-8f44-c0935b276285\", \"name\": \"Train Test Data Split\", \"status\": \"Finished\", \"start_time\": \"2020-11-14T23:40:29.763776Z\", \"created_time\": \"2020-11-14T23:40:29.763776Z\", \"end_time\": \"2020-11-14T23:40:29.843666Z\", \"duration\": \"0:00:00\", \"run_number\": 58, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-11-14T23:40:29.763776Z\", \"is_reused\": \"Yes\"}, {\"run_id\": \"c6d666a7-89c6-46fd-b6c8-f33f3cc8aebc\", \"name\": \"Transform Data\", \"status\": \"Finished\", \"start_time\": \"2020-11-14T23:40:29.149805Z\", \"created_time\": \"2020-11-14T23:40:29.149805Z\", \"end_time\": \"2020-11-14T23:40:29.22941Z\", \"duration\": \"0:00:00\", \"run_number\": 57, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-11-14T23:40:29.149805Z\", \"is_reused\": \"Yes\"}, {\"run_id\": \"f6bcae15-977b-4844-8cd9-5c2cd087a3ed\", \"name\": \"Filter Data\", \"status\": \"Finished\", \"start_time\": \"2020-11-14T23:40:28.436431Z\", \"created_time\": \"2020-11-14T23:40:28.436431Z\", \"end_time\": \"2020-11-14T23:40:28.530146Z\", \"duration\": \"0:00:00\", \"run_number\": 56, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-11-14T23:40:28.436431Z\", \"is_reused\": \"Yes\"}, {\"run_id\": \"bc650290-8c50-4811-8c4b-b9a57afd980c\", \"name\": \"Clean Data\", \"status\": \"Finished\", \"start_time\": \"2020-11-14T23:40:27.76586Z\", \"created_time\": \"2020-11-14T23:40:27.76586Z\", \"end_time\": \"2020-11-14T23:40:27.856499Z\", \"duration\": \"0:00:00\", \"run_number\": 55, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-11-14T23:40:27.76586Z\", \"is_reused\": \"Yes\"}]}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.18.0\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineRunId: 81b72bc1-2268-466c-ba06-27407046dd79\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/HyperDrive-Pipeline/runs/81b72bc1-2268-466c-ba06-27407046dd79?wsid=/subscriptions/94e14ad4-bf97-47e8-aae0-f9b85a7befa8/resourcegroups/aml-quickstarts-126184/workspaces/quick-starts-ws-126184\n",
      "{'runId': '81b72bc1-2268-466c-ba06-27407046dd79', 'status': 'Completed', 'startTimeUtc': '2020-11-14T23:40:24.060557Z', 'endTimeUtc': '2020-11-14T23:51:04.569113Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{}'}, 'inputDatasets': [], 'outputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://mlstrg126184.blob.core.windows.net/azureml/ExperimentRun/dcid.81b72bc1-2268-466c-ba06-27407046dd79/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=NhaJi3VVPRcDHmxXv1Hh3NoSE9a32W%2By%2BGI9AR%2FLQyg%3D&st=2020-11-14T23%3A30%3A46Z&se=2020-11-15T07%3A40%3A46Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://mlstrg126184.blob.core.windows.net/azureml/ExperimentRun/dcid.81b72bc1-2268-466c-ba06-27407046dd79/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=9j4wx2Hny%2Fehh2uHnz05sDPjnGZEcAf%2BhIzt2A8PCDA%3D&st=2020-11-14T23%3A30%3A46Z&se=2020-11-15T07%3A40%3A46Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://mlstrg126184.blob.core.windows.net/azureml/ExperimentRun/dcid.81b72bc1-2268-466c-ba06-27407046dd79/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=nbECCSOw2IiVVht%2BhzJ2Jd2l2GCiWkLFecKVoz7Rovg%3D&st=2020-11-14T23%3A30%3A46Z&se=2020-11-15T07%3A40%3A46Z&sp=r'}}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before we proceed we need to wait for the run to complete.\n",
    "pipeline_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "TODO: Get data. In the cell below, write code to access the data you will be using in this project. Remember that the dataset needs to be external."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598531917374
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "experiment_name = 'your experiment name here'\n",
    "\n",
    "experiment=Experiment(ws, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598531923519
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Hyperdrive Configuration\n",
    "\n",
    "TODO: Explain the model you are using and the reason for chosing the different hyperparameters, termination policy and config settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598544893076
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create an early termination policy. This is not required if you are using Bayesian sampling.\n",
    "early_termination_policy = <your policy here>\n",
    "\n",
    "#TODO: Create the different params that you will be using during training\n",
    "param_sampling = <your params here>\n",
    "\n",
    "#TODO: Create your estimator and hyperdrive config\n",
    "estimator = <your estimator here>\n",
    "\n",
    "hyperdrive_run_config = <your config here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598544897941
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Submit your experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598544898497
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Run Details\n",
    "\n",
    "OPTIONAL: Write about the different models trained and their performance. Why do you think some models did better than others?\n",
    "\n",
    "TODO: In the cell below, use the `RunDetails` widget to show the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598546648408
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model\n",
    "\n",
    "TODO: In the cell below, get the best model from the hyperdrive experiments and display all the properties of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598546650307
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598546657829
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Save the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Model Deployment\n",
    "\n",
    "Remember you have to deploy only one of the two models you trained.. Perform the steps in the rest of this notebook only if you wish to deploy this model.\n",
    "\n",
    "TODO: In the cell below, register the model, create an inference config and deploy the model as a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: In the cell below, send a request to the web service you deployed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: In the cell below, print the logs of the web service and delete the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
