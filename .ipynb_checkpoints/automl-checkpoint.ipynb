{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated ML\n",
    "\n",
    "TODO: Import Dependencies. In the cell below, import all the dependencies that you will need to complete the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1598423888013
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.18.0\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.pipeline.steps import AutoMLStep\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import pkg_resources\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, boxcox_normmax\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset\n",
    "\n",
    "### 1.1 Overview\n",
    "\n",
    "âœ… In this notebook we are going to use the Cardiovascular Disease dataset from Kaggle. Cardiovascular Disease dataset is a Kaggle Dataset the containts history of health status of some persons. A group of them suffered a heart attackt. So using this dataset we can train a model in order to predict if a person could suffer a heart attack.\n",
    "\n",
    "We can download the data from Kaggle page (https://www.kaggle.com/sulianova/cardiovascular-disease-dataset). In this case, I've download the data in the /data directory. So then we have to register this Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18393</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20228</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  \\\n",
       "0   0  18393       2     168    62.0    110     80            1     1      0   \n",
       "1   1  20228       1     156    85.0    140     90            3     1      0   \n",
       "\n",
       "   alco  active  cardio  \n",
       "0     0       1       0  \n",
       "1     0       1       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileCardioData = 'kaggle/cardio_train.csv'\n",
    "df = pd.read_csv(fileCardioData, encoding='latin')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to local folder\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "dataDir = 'data'\n",
    "\n",
    "if not os.path.exists(dataDir):\n",
    "    os.mkdir(dataDir)\n",
    "\n",
    "fileData = dataDir + \"/initialfile.parquet\"\n",
    "\n",
    "df.to_csv(fileData, index=False)\n",
    "\n",
    "print(\"Data written to local folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Upload to Azure Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace: quick-starts-ws-125985\n",
      "Region: southcentralus\n",
      "Uploading an estimated of 1 files\n",
      "Uploading data/initialfile.parquet\n",
      "Uploaded data/initialfile.parquet, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n",
      "Upload completed\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(\"Workspace: \" + ws.name, \"Region: \" + ws.location, sep = '\\n')\n",
    "\n",
    "# Default datastore\n",
    "default_store = ws.get_default_datastore() \n",
    "\n",
    "default_store.upload_files([fileData], \n",
    "                           target_path = 'cardio', \n",
    "                           overwrite = True, \n",
    "                           show_progress = True)\n",
    "\n",
    "print(\"Upload completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create and register datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "cardio_data = Dataset.Tabular.from_delimited_files(default_store.path('cardio/initialfile.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardio_data = cardio_data.register(ws, 'cardio_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "amlcompute_cluster_name = \"compt-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    aml_compute = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           max_nodes=4)\n",
    "    aml_compute = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
    "\n",
    "aml_compute.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run configuration created.\n"
     ]
    }
   ],
   "source": [
    "# Define RunConfig for the compute\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Create a new runconfig object\n",
    "aml_run_config = RunConfiguration()\n",
    "\n",
    "# Use the aml_compute you created above. \n",
    "aml_run_config.target = aml_compute\n",
    "\n",
    "# Enable Docker\n",
    "aml_run_config.environment.docker.enabled = True\n",
    "\n",
    "# Use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "aml_run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# Specify CondaDependencies obj, add necessary packages\n",
    "aml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\n",
    "    conda_packages=['pandas','scikit-learn','numpy'], \n",
    "    pip_packages=['azureml-sdk[automl,explain]', 'scipy'])\n",
    "\n",
    "print (\"Run configuration created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial columns to use\n",
    "cols_touse = str(['age', 'height', 'weight', 'ap_hi', 'ap_lo',\n",
    "       'cholesterol', 'gluc', 'smoke', 'alco', 'active', 'cardio']).replace(\",\", \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning script is in /mnt/batch/tasks/shared/LS_root/mounts/clusters/compt-ins/code/nd00333_AZMLND_Capstone_Project/scripts.\n",
      "Clean Step created\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "# python scripts folder\n",
    "prepare_data_folder = './scripts'\n",
    "\n",
    "# Define output after cleansing step\n",
    "cleaned_data = PipelineData(\"cleaned_data\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Cleaning script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# Cleaning step creation\n",
    "cleaningStep = PythonScriptStep(\n",
    "    name=\"Clean Data\",\n",
    "    script_name=\"clean.py\", \n",
    "    arguments=[\"--useful_columns\", cols_touse,\n",
    "               \"--output_clean\", cleaned_data],\n",
    "    inputs=[cardio_data.as_named_input('raw_data')],\n",
    "    outputs=[cleaned_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig=aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"Clean Step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter script is in /mnt/batch/tasks/shared/LS_root/mounts/clusters/compt-ins/code/nd00333_AZMLND_Capstone_Project/scripts.\n",
      "Filter Step created\n"
     ]
    }
   ],
   "source": [
    "# Define output after merging step\n",
    "filtered_data = PipelineData(\"filtered_data\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Filter script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# filter step creation\n",
    "# See the filter.py for details about input and output\n",
    "filterStep = PythonScriptStep(\n",
    "    name=\"Filter Data\",\n",
    "    script_name=\"filter.py\", \n",
    "    arguments=[\"--output_filter\", filtered_data],\n",
    "    inputs=[cleaned_data],\n",
    "    outputs=[filtered_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig = aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"Filter Step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform script is in /mnt/batch/tasks/shared/LS_root/mounts/clusters/compt-ins/code/nd00333_AZMLND_Capstone_Project/scripts.\n",
      "Transform Step created\n"
     ]
    }
   ],
   "source": [
    "# Define output after transform step\n",
    "transformed_data = PipelineData(\"transformed_data\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Transform script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# transform step creation\n",
    "# See the transform.py for details about input and output\n",
    "transformStep = PythonScriptStep(\n",
    "    name=\"Transform Data\",\n",
    "    script_name=\"transform.py\", \n",
    "    arguments=[\"--output_transform\", transformed_data],\n",
    "    inputs=[filtered_data],\n",
    "    outputs=[transformed_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig = aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"Transform Step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Split Data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data spilt script is in /mnt/batch/tasks/shared/LS_root/mounts/clusters/compt-ins/code/nd00333_AZMLND_Capstone_Project/scripts.\n",
      "TrainTest Split Step created\n"
     ]
    }
   ],
   "source": [
    "# train and test splits output\n",
    "output_split_train = PipelineData(\"output_split_train\", datastore=default_store).as_dataset()\n",
    "output_split_test = PipelineData(\"output_split_test\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Data spilt script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# test train split step creation\n",
    "# See the train_test_split.py for details about input and output\n",
    "testTrainSplitStep = PythonScriptStep(\n",
    "    name=\"Train Test Data Split\",\n",
    "    script_name=\"train_test_split.py\", \n",
    "    arguments=[\"--output_split_train\", output_split_train,\n",
    "               \"--output_split_test\", output_split_test],\n",
    "    inputs=[transformed_data],\n",
    "    outputs=[output_split_train, output_split_test],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig = aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"TrainTest Split Step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 AutoML Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment created\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment = Experiment(ws, 'AutoML-Pipeline')\n",
    "\n",
    "print(\"Experiment created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML config created\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "automl_settings = {\n",
    "    \"experiment_timeout_minutes\": 30,\n",
    "    \"max_concurrent_iterations\": 5,\n",
    "    \"primary_metric\" : 'AUC_weighted',\n",
    "    \"n_cross_validations\": 5\n",
    "}\n",
    "\n",
    "training_dataset = output_split_train.parse_parquet_files()\n",
    "\n",
    "automl_config = AutoMLConfig(compute_target=aml_compute,\n",
    "                             task = \"classification\",\n",
    "                             training_data=training_dataset,\n",
    "                             label_column_name=\"cardio\",  \n",
    "                             path = prepare_data_folder,\n",
    "                             enable_early_stopping= True,\n",
    "                             featurization= 'auto',#\n",
    "                             debug_log = \"automl_errors.log\",\n",
    "                             **automl_settings\n",
    "                            )\n",
    "\n",
    "print(\"AutoML config created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData, TrainingOutput\n",
    "\n",
    "ds = ws.get_default_datastore()\n",
    "metrics_output_name = 'metrics_output'\n",
    "best_model_output_name = 'best_model_output'\n",
    "\n",
    "metrics_data = PipelineData(name='metrics_data',\n",
    "                           datastore=ds,\n",
    "                           pipeline_output_name=metrics_output_name,\n",
    "                           training_output=TrainingOutput(type='Metrics'))\n",
    "model_data = PipelineData(name='model_data',\n",
    "                           datastore=ds,\n",
    "                           pipeline_output_name=best_model_output_name,\n",
    "                           training_output=TrainingOutput(type='Model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainWithAutomlStep created\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.steps import AutoMLStep\n",
    "\n",
    "trainWithAutomlStep = AutoMLStep(name='AutoML_Classification',\n",
    "                                 outputs=[metrics_data, model_data],\n",
    "                                 automl_config=automl_config,\n",
    "                                 allow_reuse=True)\n",
    "print(\"trainWithAutomlStep created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline is built.\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "pipeline_steps = [trainWithAutomlStep]\n",
    "\n",
    "pipeline = Pipeline(workspace = ws, steps=pipeline_steps)\n",
    "print(\"Pipeline is built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = experiment.submit(pipeline, regenerate_outputs=False)\n",
    "\n",
    "print(\"Pipeline submitted for execution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we proceed we need to wait for the run to complete.\n",
    "pipeline_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML Configuration\n",
    "\n",
    "TODO: Explain why you chose the automl settings and cofiguration you used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598429217746
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Put your automl settings here\n",
    "automl_settings = {}\n",
    "\n",
    "# TODO: Put your automl config here\n",
    "automl_config = AutoMLConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431107951
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Submit your experiment\n",
    "remote_run = experiment.submit(automl_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Details\n",
    "\n",
    "OPTIONAL: Write about the different models trained and their performance. Why do you think some models did better than others?\n",
    "\n",
    "TODO: In the cell below, use the `RunDetails` widget to show the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431121770
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model\n",
    "\n",
    "TODO: In the cell below, get the best model from the automl experiments and display all the properties of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431425670
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431426111
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Save the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "Remember you have to deploy only one of the two models you trained.. Perform the steps in the rest of this notebook only if you wish to deploy this model.\n",
    "\n",
    "TODO: In the cell below, register the model, create an inference config and deploy the model as a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431435189
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598431657736
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, send a request to the web service you deployed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598432707604
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598432765711
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, print the logs of the web service and delete the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
